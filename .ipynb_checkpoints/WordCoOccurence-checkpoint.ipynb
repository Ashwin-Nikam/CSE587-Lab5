{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-898a6c308479>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-898a6c308479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ashwin/spark-1.2.0-bin-hadoop2.4/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/ashwin/spark-1.2.0-bin-hadoop2.4/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 228\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-898a6c308479>:2 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for importing lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "new_dict = dict()\n",
    "def readfile(filename):\n",
    "    count = 0\n",
    "    with open(filename) as f:\n",
    "        csvr = csv.reader(f)\n",
    "        for line in csvr:\n",
    "            samp = \"\"\n",
    "            for i in range(1,len(line)):\n",
    "                if(line[i] != ''):\n",
    "                    samp += line[i] + ','\n",
    "            new_dict[line[0]] = samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readfile('new_lemmatizer.csv')\n",
    "text_file = sc.textFile(\"co_occur_input\")\n",
    "text_file = text_file.filter(lambda x: x is not u'')  #To remove blank lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mainReplace(word):\n",
    "\tword = word.replace(\",\",\"\")\n",
    "\tword = word.replace(\"!\",\"\")\n",
    "\tword = word.replace(\"?\",\"\")\n",
    "\tword = word.replace(\":\",\"\")\n",
    "\tword = word.replace(\"#\",\"\")\n",
    "\tword = word.replace(\"\\u\",\"\")\n",
    "\treturn word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main bigram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainMethod(line):\n",
    "    words = line[0]\n",
    "    pos = line[1]\n",
    "    list = []\n",
    "    one = mainReplace(words[0])\n",
    "    two = mainReplace(words[1])\n",
    "    if new_dict.has_key(one) and new_dict.has_key(two):    \n",
    "        a = new_dict[one]\n",
    "        b = new_dict[two]\n",
    "        a = a.split(',')\n",
    "        b = b.split(',')\n",
    "        a.remove('')\n",
    "        b.remove('')\n",
    "        for i in a:\n",
    "            for j in b:\n",
    "                list.append(((i,j),pos))\n",
    "    elif new_dict.has_key(one):\n",
    "        a = new_dict[one]\n",
    "        a = a.split(',')\n",
    "        a.remove('')\n",
    "        for i in a:\n",
    "            list.append(((i, two),pos))\n",
    "    elif new_dict.has_key(two):\n",
    "        b = new_dict[two]\n",
    "        b = b.split(',')\n",
    "        b.remove('')\n",
    "        for j in b:\n",
    "            list.append(((one, j),pos))\n",
    "    else:\n",
    "        list.append(((one,two),pos))\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate 2 grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "rep = {'j':'i', 'v':'u'}\n",
    "low = text_file.map(lambda line: line.lower())\n",
    "text = low.map(lambda line : (line.split('>')[0]+\">\",replace_all(line.split('>')[1].lstrip(),rep).split()))\n",
    "bigram = text.flatMap(lambda line:  [(pair, line[0]) for pair in itertools.combinations(line[1], 2)])\n",
    "reduced = bigram.reduceByKey(lambda a,b: a+\" \"+b)\n",
    "lemmatize = reduced.flatMap(mainMethod)\n",
    "lemmatize.saveAsTextFile('paircount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main trigram method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mainMethod1(line):\n",
    "    words = line[0]\n",
    "    pos = line[1]\n",
    "    list = []\n",
    "    one = mainReplace(words[0])\n",
    "    two = mainReplace(words[1])\n",
    "    three = mainReplace(words[2])\n",
    "    if new_dict.has_key(one) and new_dict.has_key(two) and new_dict.has_key(three):    \n",
    "        a = new_dict[one]\n",
    "        a = a.split(',')\n",
    "        a.remove('')\n",
    "        b = new_dict[two]\n",
    "        b = b.split(',')\n",
    "        b.remove('')\n",
    "        c = new_dict[three]\n",
    "        c = c.split(',')\n",
    "        c.remove('')\n",
    "        for i in a:\n",
    "            for j in b:\n",
    "                for k in c:\n",
    "                    list.append(((i,j,k),pos))\n",
    "    elif new_dict.has_key(one) and new_dict.has_key(two):\n",
    "        a = new_dict[one]\n",
    "        a = a.split(',')\n",
    "        a.remove('')\n",
    "        b = new_dict[two]\n",
    "        b = b.split(',')\n",
    "        b.remove('')\n",
    "        for i in a:\n",
    "            for j in b:\n",
    "                list.append(((i, j, three),pos))\n",
    "    elif new_dict.has_key(one) and new_dict.has_key(three):\n",
    "        a = new_dict[one]\n",
    "        a = a.split(',')\n",
    "        a.remove('')\n",
    "        c = new_dict[three]\n",
    "        c = c.split(',')\n",
    "        c.remove('')\n",
    "        for i in a:\n",
    "            for k in c:\n",
    "                list.append(((i, two, k),pos))\n",
    "    elif new_dict.has_key(two) and new_dict.has_key(three):\n",
    "        b = new_dict[two]\n",
    "        b = b.split(',')\n",
    "        b.remove('')\n",
    "        c = new_dict[three]\n",
    "        c = c.split(',')\n",
    "        c.remove('')\n",
    "        for j in b:\n",
    "            for k in c:\n",
    "                list.append(((one,j,k),pos))\n",
    "    elif new_dict.has_key(one):\n",
    "        a = new_dict[one]\n",
    "        a = a.split(',')\n",
    "        a.remove('')\n",
    "        for i in a:\n",
    "            list.append(((i,two,three),pos))\n",
    "    elif new_dict.has_key(two):\n",
    "        b = new_dict[two]\n",
    "        b = b.split(',')\n",
    "        b.remove('')\n",
    "        for j in b:\n",
    "            list.append(((one,j,three),pos))\n",
    "    elif new_dict.has_key(three):\n",
    "        c = new_dict[three]\n",
    "        c = c.split(',')\n",
    "        c.remove('')\n",
    "        for k in c:\n",
    "            list.append(((one,two,k),pos))\n",
    "    else:\n",
    "        list.append(((one,two,three),pos))\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to calculate 3 grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "rep = {'j':'i', 'v':'u'}\n",
    "low = text_file.map(lambda line: line.lower())\n",
    "text = low.map(lambda line : (line.split('>')[0]+\">\",replace_all(line.split('>')[1].lstrip(),rep).split()))\n",
    "trigram = text.flatMap(lambda line:  [(triple, line[0]) for triple in itertools.combinations(line[1], 3)])\n",
    "reduced = trigram.reduceByKey(lambda a,b: a+\" \"+b)\n",
    "lemmatize = reduced.flatMap(mainMethod1)\n",
    "lemmatize.saveAsTextFile('tricount')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
